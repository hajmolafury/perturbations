{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import trange\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tfds.image.MNIST()\n",
    "\n",
    "mnist.download_and_prepare()\n",
    "\n",
    "datasets = mnist.as_dataset()\n",
    "train_dataset, test_dataset = datasets['train'], datasets['test']\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset=train_dataset.prefetch(1)\n",
    "# Prefetch data for faster consumption:\n",
    "#.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(1)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_initializable_iterator(train_dataset, name=)\n",
    "\n",
    "#* Iterator returns nested Tensors in the form of a dictionary with keys as 'image' and 'label' of size batch_size\n",
    "\n",
    "batch_data = iterator.get_next()\n",
    "images=tf.keras.layers.Flatten()(tf.dtypes.cast(batch_data['image'], tf.float32)/255)\n",
    "labels=tf.one_hot(batch_data['label'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_hl1=300\n",
    "nodes_hl2=300\n",
    "n_classes=10\n",
    "\n",
    "  \n",
    "initializer= tf.compat.v1.initializers.glorot_normal()\n",
    "\n",
    "hl1={'weights':tf.Variable(initializer([784,nodes_hl1])),'bias':tf.Variable(initializer([nodes_hl1]))}\n",
    "hl2={'weights':tf.Variable(initializer([nodes_hl1,nodes_hl2])),'bias':tf.Variable(initializer([nodes_hl2]))}\n",
    "output_layer={'weights':tf.Variable(initializer([nodes_hl2,n_classes])),'bias':tf.Variable(initializer([n_classes]))}\n",
    "\n",
    "\n",
    "l1=tf.add(tf.matmul(images,hl1['weights']),hl1['bias'])\n",
    "l1=tf.nn.relu(l1)\n",
    "                                                                            \n",
    "l2=tf.add(tf.matmul(l1,hl2['weights']),hl2['bias'])\n",
    "l2=tf.nn.relu(l2)\n",
    "                                                                                \n",
    "logits=tf.add(tf.matmul(l2,output_layer['weights']),output_layer['bias'])\n",
    "                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 600/600 [00:07<00:00, 82.59it/s, loss=0.072]\n100%|██████████| 600/600 [00:07<00:00, 77.81it/s, loss=0.031]\n"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# get accuracy\n",
    "pred = tf.argmax(logits, 1)\n",
    "equality = tf.equal(pred, tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "init_op = tf.global_variables_initializer()\n",
    "# run the training\n",
    "epochs=2\n",
    "n_train=60000\n",
    "batches = int(n_train/batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        t=trange(batches)\n",
    "        for j in t:\n",
    "            _,loss_i=sess.run([optimizer, loss])\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}